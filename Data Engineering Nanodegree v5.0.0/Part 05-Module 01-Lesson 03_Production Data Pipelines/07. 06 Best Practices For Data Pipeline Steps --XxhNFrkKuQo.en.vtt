WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.580
Once you start to build your own DAGs in Airflow,

00:00:02.580 --> 00:00:06.810
you may begin to wonder what the right boundaries are between tasks.

00:00:06.809 --> 00:00:08.429
In our bike share example,

00:00:08.429 --> 00:00:11.564
tasks within the DAG have been defined for you.

00:00:11.564 --> 00:00:16.844
Have you stopped to wonder why we have two S3 to Redshift load operations?

00:00:16.844 --> 00:00:20.054
Why do we have two data quality checks instead of just one?

00:00:20.054 --> 00:00:21.539
Why not just do them together?

00:00:21.539 --> 00:00:25.184
Though it may seem obvious with the DAG sketched out in front of you,

00:00:25.184 --> 00:00:28.859
in practice, finding these boundaries can be harder than you might think.

00:00:28.859 --> 00:00:31.230
In this section, we'll cover some good rules to

00:00:31.230 --> 00:00:34.710
use when reasoning about how to break apart tasks.

00:00:34.710 --> 00:00:38.524
Every task in your DAG should perform only one job.

00:00:38.524 --> 00:00:41.460
To paraphrase Ken Thompson's Unix philosophy,

00:00:41.460 --> 00:00:45.109
write programs that do one thing, and do it well.

00:00:45.109 --> 00:00:48.320
We can apply the same concept in writing the task within

00:00:48.320 --> 00:00:53.119
our DAGs: Write tasks that do one thing, and do it well.

00:00:53.119 --> 00:00:57.169
This is useful for you as a developer as you build your data pipelines.

00:00:57.170 --> 00:01:00.920
If you revisit a pipeline you wrote six months ago,

00:01:00.920 --> 00:01:05.359
you'll have a much easier time understanding how it works in

00:01:05.359 --> 00:01:10.219
the lineage of the data if the boundaries between tasks are clear and well defined.

00:01:10.219 --> 00:01:14.435
This is true in the code itself and with an Airflow UI.

00:01:14.435 --> 00:01:16.010
So, you can see here,

00:01:16.010 --> 00:01:18.035
we have just a single task.

00:01:18.034 --> 00:01:22.674
It's pretty descriptive as to what it's doing: copying data from S3 to Redshift,

00:01:22.674 --> 00:01:24.649
and it gives us a pretty clear idea of

00:01:24.650 --> 00:01:27.380
exactly the operations that are happening within the task.

00:01:27.379 --> 00:01:29.509
If we did three or four other things here,

00:01:29.510 --> 00:01:32.810
this wouldn't make as much sense and it wouldn't be obvious what it was doing.

00:01:32.810 --> 00:01:37.325
Tasks that do just one thing are often more easily parallelized.

00:01:37.325 --> 00:01:42.859
This parallelization can offer a significant speedup in the execution of our DAGs.

00:01:42.859 --> 00:01:46.135
We've already observed this effect in our bike share DAG.

00:01:46.135 --> 00:01:49.910
You can see these two tasks are running at the same time.

00:01:49.909 --> 00:01:52.700
You may have noticed that in the final exercise of Lesson

00:01:52.700 --> 00:01:57.070
two that multiple copy and data quality checks could run at once.

00:01:57.069 --> 00:02:01.250
This is because there are not cross-dependencies between these data sets.

00:02:01.250 --> 00:02:03.454
If we had additional data sets,

00:02:03.454 --> 00:02:06.469
we could add more copies and data quality checks.

00:02:06.469 --> 00:02:10.145
In Airflow, we parallelize those copies and data quality checks as well.

00:02:10.145 --> 00:02:12.830
One other thing I just want to emphasize here in this diagram,

00:02:12.830 --> 00:02:17.675
so if this is our load trips and load stations task, when properly parallelized,

00:02:17.675 --> 00:02:19.460
if we have time equals zero,

00:02:19.460 --> 00:02:20.945
so this is zero seconds,

00:02:20.944 --> 00:02:22.519
this is six seconds,

00:02:22.520 --> 00:02:24.310
and this is 10 seconds,

00:02:24.310 --> 00:02:27.365
Airflow will run both of these things at the same time.

00:02:27.365 --> 00:02:31.939
So, the total duration will be whatever the longest running task is.

00:02:31.939 --> 00:02:33.199
So, in this case,

00:02:33.199 --> 00:02:35.164
load stations took six seconds,

00:02:35.164 --> 00:02:36.829
load trips took 10 seconds.

00:02:36.830 --> 00:02:41.040
So, the total execution for both of them will be about 10 seconds.

00:02:41.039 --> 00:02:45.099
If instead we bundle all of our data loading and quality checks into just a few tasks,

00:02:45.099 --> 00:02:47.644
it could take much longer to run our DAG.

00:02:47.645 --> 00:02:52.915
So, if for whatever reason we decided to put these two tasks together into a single task,

00:02:52.914 --> 00:02:55.219
then we would get the worst of both worlds.

00:02:55.219 --> 00:02:57.395
Not only would we lose visibility,

00:02:57.395 --> 00:02:59.930
we would also see that the execution time would go up.

00:02:59.930 --> 00:03:04.040
So, again, if this is zero seconds and this is 16 seconds,

00:03:04.039 --> 00:03:07.254
we know from our previous slide that load trips takes about 10 seconds.

00:03:07.254 --> 00:03:12.740
If I had load stations running the same task after that and it took six seconds,

00:03:12.740 --> 00:03:16.100
we would have a total of 16 seconds of execution time.

00:03:16.099 --> 00:03:17.734
When a task fails,

00:03:17.735 --> 00:03:21.250
it's important to quickly identify the issue and resolve the problem.

00:03:21.250 --> 00:03:24.164
When each task in your DAG performs a single purpose,

00:03:24.164 --> 00:03:29.044
it's easy to look at the Airflow UI to determine what exactly failed.

00:03:29.044 --> 00:03:32.329
However, if your tasks are performing more than one task each,

00:03:32.330 --> 00:03:34.715
you'll often struggle to figure out what broke.

00:03:34.715 --> 00:03:37.789
Again, if in our bike share example we had loaded

00:03:37.789 --> 00:03:41.629
both the trips and stations data at the same time in a single task,

00:03:41.629 --> 00:03:45.394
how do we know which data set had failed to load if that task failed?

00:03:45.395 --> 00:03:47.030
Was it the trips data set?

00:03:47.030 --> 00:03:48.740
Was it the stations data set?

00:03:48.740 --> 00:03:52.100
Maybe both of them failed to load. It's not clear.

00:03:52.099 --> 00:03:56.599
Well-defined boundaries make our tasks more maintainable and easy to debug.

00:03:56.599 --> 00:03:59.359
You can see in this example here that

00:03:59.360 --> 00:04:02.810
Airflow uses the dark red to indicate that a task has failed.

00:04:02.810 --> 00:04:05.909
The dark yellow means that an upstream task that

00:04:05.909 --> 00:04:09.734
this task depends on has failed and this task can no longer run.

00:04:09.735 --> 00:04:14.540
So, load trips from S3 to Redshift depends on the create trips table.

00:04:14.539 --> 00:04:16.639
So, if the create trips table has failed,

00:04:16.639 --> 00:04:18.750
then we can't run this task.

