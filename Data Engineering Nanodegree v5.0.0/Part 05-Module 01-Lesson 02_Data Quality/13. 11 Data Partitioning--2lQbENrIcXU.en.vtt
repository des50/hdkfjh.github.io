WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.419
Now, we'll learn more about data partitioning and data pipelines

00:00:03.419 --> 00:00:07.064
and how a good data partitioning strategy can make our pipelines faster,

00:00:07.065 --> 00:00:09.509
more reliable and easier to test.

00:00:09.509 --> 00:00:12.990
First, let's define what data partitioning is.

00:00:12.990 --> 00:00:16.050
Data partitioning is the process of isolating data to

00:00:16.050 --> 00:00:19.140
be analyzed by one or more attributes such as time,

00:00:19.140 --> 00:00:21.074
which we just reviewed in the previous lesson,

00:00:21.074 --> 00:00:23.939
logical type, or data size.

00:00:23.940 --> 00:00:28.125
In the previous lecture, we spoke in detail about pipelines schedules,

00:00:28.125 --> 00:00:30.464
which are a form of time partitioning.

00:00:30.464 --> 00:00:32.700
Not only are schedules great for reducing

00:00:32.700 --> 00:00:35.174
the amount of data our pipelines have to process,

00:00:35.173 --> 00:00:37.530
but they also help us guarantee that we can be

00:00:37.530 --> 00:00:40.125
timing guarantees that our data consumers may need.

00:00:40.125 --> 00:00:45.590
In this section, we'll learn more about partitioning by logical type or data size.

00:00:45.590 --> 00:00:48.080
We'll also explore the reasons why data partitioning is

00:00:48.079 --> 00:00:51.695
such an important consideration in the design and creation of our pipelines.

00:00:51.695 --> 00:00:54.590
Conceptually related data can be partitioned into

00:00:54.590 --> 00:00:57.590
discrete segments and then processed further from there.

00:00:57.590 --> 00:00:59.960
This process of separating data based on

00:00:59.960 --> 00:01:03.460
this conceptual relationship is called logical partitioning.

00:01:03.460 --> 00:01:05.019
In our partial example,

00:01:05.019 --> 00:01:07.444
we've loaded trip and stations data.

00:01:07.444 --> 00:01:10.009
We have tripped data and stations data.

00:01:10.010 --> 00:01:11.870
But we loaded them independently.

00:01:11.870 --> 00:01:13.939
So, we have two tasks.

00:01:13.939 --> 00:01:15.230
We have an estimator redshift for

00:01:15.230 --> 00:01:18.545
the trip data and an estimator redshift for the stations data.

00:01:18.545 --> 00:01:20.900
If we loaded both trip and stations data

00:01:20.900 --> 00:01:25.185
together at the same time and one or both of those tests failed,

00:01:25.185 --> 00:01:29.900
it would be hard to debug than if we had just loaded each data set independently.

00:01:29.900 --> 00:01:31.940
That being said, if we were to analyze

00:01:31.939 --> 00:01:35.420
both our trips data and stations data to produce a single output.

00:01:35.420 --> 00:01:38.600
So, let's say that this was going to create one output.

00:01:38.599 --> 00:01:41.914
That would be a totally valid thing to do to combine them.

00:01:41.915 --> 00:01:45.080
With logical partitioning, the thing to keep in mind is

00:01:45.079 --> 00:01:48.575
that unrelated things belong in separate steps.

00:01:48.575 --> 00:01:53.790
Consider your dependencies and separate processing around those boundaries.

00:01:54.909 --> 00:01:58.534
So, as partitioning separates data for processing

00:01:58.534 --> 00:02:01.340
based on desired or required storage limits.

00:02:01.340 --> 00:02:03.094
So, in this example,

00:02:03.093 --> 00:02:07.459
we have our estimated redshift operator and we might actually only want to look at say,

00:02:07.459 --> 00:02:10.340
one gigabyte of data at a time.

00:02:10.340 --> 00:02:15.289
Many events streaming systems such as Amazon's Firehose can be configured to

00:02:15.289 --> 00:02:19.894
write data out to key stores like Amazon S3 when a particular size has been hit.

00:02:19.895 --> 00:02:22.295
This is particularly important with airflow as

00:02:22.294 --> 00:02:26.344
airflow workers will have limited amount of memory available for processing.

00:02:26.344 --> 00:02:28.189
As we reviewed in Lesson one,

00:02:28.189 --> 00:02:30.079
airflow workers only have the amount of

00:02:30.080 --> 00:02:32.990
memory on their individual machine available to them.

00:02:32.990 --> 00:02:36.590
They can't pull memory like systems such as Apache Spark.

00:02:36.590 --> 00:02:38.164
In the above diagram,

00:02:38.164 --> 00:02:43.250
the airflow test running on worker is handling one gigabyte of data at a time.

00:02:43.250 --> 00:02:47.900
We can probably safely assume that our workers have more than one gigabyte of RAM.

00:02:47.900 --> 00:02:50.765
So, we can be certainly can process this much data.

00:02:50.764 --> 00:02:54.589
Additionally, processing the small amount of data maybe much faster

00:02:54.590 --> 00:02:58.870
than handling all of data available all at once.

00:02:58.870 --> 00:03:01.939
Size partitioning is critical to understand when working

00:03:01.939 --> 00:03:04.669
with large data sets, especially with airflow.

00:03:04.669 --> 00:03:08.534
Airflow workers don't pull memory like some data processing frameworks.

00:03:08.534 --> 00:03:11.960
If your airflow worker has 32 gigabytes of RAM,

00:03:11.960 --> 00:03:15.349
which is substantial for some machines that you might actually see

00:03:15.349 --> 00:03:18.889
used and you tried to load a 100 gigabyte data set,

00:03:18.889 --> 00:03:21.164
the worker is going to kill the task.

00:03:21.164 --> 00:03:22.849
If the worker kills a task,

00:03:22.849 --> 00:03:25.549
you'll be unable to analyze that data and will likely have

00:03:25.550 --> 00:03:28.580
to redesign your DAG or repartition your data.

00:03:28.580 --> 00:03:31.430
This is one of the reasons we recommend you use airflow to trigger

00:03:31.430 --> 00:03:34.865
dataframe works like Spark to process large data sets.

00:03:34.865 --> 00:03:38.930
The reason that I say you may have to redesign your DAG is because if

00:03:38.930 --> 00:03:44.000
you built your DAG with the assumption that it can handle all the data in memory,

00:03:44.000 --> 00:03:45.979
the likelihood is your code's going to have to

00:03:45.979 --> 00:03:48.774
change to read only a little bit of data at a time.

00:03:48.775 --> 00:03:50.980
So, in these types of situations,

00:03:50.979 --> 00:03:52.729
we even need to partition the data by

00:03:52.729 --> 00:03:56.060
size into say those one gigabyte chunks like you saw on

00:03:56.060 --> 00:03:59.284
that first slide or you may want to trigger

00:03:59.284 --> 00:04:04.719
an external processing framework like Spark to actually work with these larger data sets.

