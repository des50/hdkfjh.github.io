WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.585
Data quality is the measure of how well a dataset satisfies its intended use.

00:00:05.585 --> 00:00:09.824
So how do we know what its intended use is? How do we measure that?

00:00:09.824 --> 00:00:12.599
When we talk about the intended use of data,

00:00:12.599 --> 00:00:14.250
we're typically referring to how

00:00:14.250 --> 00:00:18.375
our downstream data consumers are going to utilize this data.

00:00:18.375 --> 00:00:21.149
Before we can measure data quality,

00:00:21.149 --> 00:00:23.834
we need to have a set of standards to measure against.

00:00:23.835 --> 00:00:26.130
This is where requirements come in.

00:00:26.129 --> 00:00:29.399
Defining requirements for our data pipelines and collaborating with

00:00:29.399 --> 00:00:34.769
our data consumers on those requirements is how we can set and measure data quality.

00:00:34.770 --> 00:00:38.180
Let's walk through examples of the types of requirements we can

00:00:38.179 --> 00:00:41.585
expect to encounter in our day-to-day as data engineers.

00:00:41.585 --> 00:00:43.820
This list isn't meant to be exhaustive.

00:00:43.820 --> 00:00:49.115
However, it is a set of good examples of real-world data quality measurements.

00:00:49.115 --> 00:00:53.060
The first thing we're likely to consider is the size of our data,

00:00:53.060 --> 00:00:54.529
this type of requirement,

00:00:54.529 --> 00:00:58.130
at a minimum sets the expectation that data exists.

00:00:58.130 --> 00:00:59.705
In our bikeshare example,

00:00:59.704 --> 00:01:02.765
we would like to check that after we've copied our records from

00:01:02.765 --> 00:01:07.280
S3 to Redshift that we have more than one row in our trips table.

00:01:07.280 --> 00:01:08.930
If we don't have any records,

00:01:08.930 --> 00:01:11.345
then we know we violated the data size constraint.

00:01:11.344 --> 00:01:14.390
Some pipelines may take size requirements further and

00:01:14.390 --> 00:01:18.215
check the data produced does not exceed a certain size.

00:01:18.215 --> 00:01:22.314
Data accuracy is another common measure of data quality.

00:01:22.314 --> 00:01:25.120
When we produce data for our data consumers to use,

00:01:25.120 --> 00:01:27.350
we want to make sure it's meaningful.

00:01:27.349 --> 00:01:29.703
This builds trust in our data consumers,

00:01:29.703 --> 00:01:31.399
it means that we can have more confidence

00:01:31.400 --> 00:01:35.225
building on top of the insights or pipelines generate.

00:01:35.224 --> 00:01:40.219
Exactly how accuracy is measured is dependent on the application.

00:01:40.219 --> 00:01:41.644
In our bikeshare example,

00:01:41.644 --> 00:01:43.924
we calculated a station traffic table.

00:01:43.924 --> 00:01:45.480
After we've produced that table,

00:01:45.480 --> 00:01:47.570
we can perform spot checks to verify

00:01:47.569 --> 00:01:50.389
the maximum number of visits and the minimum number of

00:01:50.390 --> 00:01:52.879
visits as well as the average number of visits

00:01:52.879 --> 00:01:56.015
across those stations see with unexpected boundaries.

00:01:56.015 --> 00:02:00.620
For example, if we had 3.6 million rides in 2018 and we

00:02:00.620 --> 00:02:05.165
saw that our MAX station had some number much higher than that like 10 million,

00:02:05.165 --> 00:02:09.520
or 999 million, or fewer than say 1,000 rides,

00:02:09.520 --> 00:02:12.130
then we pretty sure something went wrong in our analysis,

00:02:12.129 --> 00:02:15.289
and the quality of our data is likely very poor.

00:02:15.289 --> 00:02:20.375
Timeliness of data is another critical component in determining data quality.

00:02:20.375 --> 00:02:24.110
Many datasets are used in a time-critical context.

00:02:24.110 --> 00:02:26.795
Maybe you want to send thank you emails to a customer

00:02:26.794 --> 00:02:29.659
or send out a batch of receipts every 15 minutes.

00:02:29.659 --> 00:02:32.150
Maybe your marketing department uses all the data from

00:02:32.150 --> 00:02:34.564
the past hour to make automated decisions

00:02:34.564 --> 00:02:36.530
about what ads to purchase in a tool like

00:02:36.530 --> 00:02:40.069
Google AdWords based on nearly real-time analytics.

00:02:40.069 --> 00:02:41.525
Suffice it to say,

00:02:41.525 --> 00:02:42.784
in both of those scenarios,

00:02:42.784 --> 00:02:46.129
if our data processing where to take longer than an expected window,

00:02:46.129 --> 00:02:48.918
the data may be of no use or worse,

00:02:48.919 --> 00:02:51.520
maybe inaccurate and lead to bad decision-making.

00:02:51.520 --> 00:02:54.105
For example, in that marketing example,

00:02:54.104 --> 00:02:59.419
if we spent a million dollars on an ad buy based on data that was hours out of date,

00:02:59.419 --> 00:03:02.139
our marketing department will not be very happy.

00:03:02.139 --> 00:03:07.714
Thankfully, Airflow make setting SLAs to fail task and DAGs that take too long simple.

00:03:07.715 --> 00:03:10.735
We'll see this in our next exercise shortly.

00:03:10.735 --> 00:03:14.750
Ensuring that our pipelines execute with a given frequency is

00:03:14.750 --> 00:03:18.979
closely related to measuring the timeliness of data as we just discussed.

00:03:18.979 --> 00:03:22.104
We reviewed pipeline scheduling earlier in the lecture,

00:03:22.104 --> 00:03:24.439
but I'll just repeat that it's typically important that

00:03:24.439 --> 00:03:28.069
analysis of data had been a particular frequency or schedule.

00:03:28.069 --> 00:03:29.889
Thankfully, as you've already know,

00:03:29.889 --> 00:03:32.419
Airflow not only takes the burden off of us to make sure that

00:03:32.419 --> 00:03:35.089
our pipelines execute on a particular schedule,

00:03:35.090 --> 00:03:39.259
but it also keeps track of when and how long each pipeline run took.

00:03:39.259 --> 00:03:40.829
So just by using Airflow,

00:03:40.830 --> 00:03:42.830
we'll already be able to easily measure and track

00:03:42.830 --> 00:03:46.130
this requirement to satisfy quality metrics.

00:03:46.129 --> 00:03:48.740
Finally, some of you may have heard of

00:03:48.740 --> 00:03:52.195
GDPR or other privacy regulations from around the world.

00:03:52.194 --> 00:03:56.269
Even when our businesses don't come under the auspices of privacy regulations,

00:03:56.270 --> 00:03:57.920
it's typically a good idea to keep

00:03:57.919 --> 00:04:01.250
as much sensitive and personally identifiable information

00:04:01.250 --> 00:04:03.259
out of our analyses as possible.

00:04:03.259 --> 00:04:07.189
This is respectful to our users and a good defensive data practice.

00:04:07.189 --> 00:04:09.590
It's increasingly common for businesses to ask

00:04:09.590 --> 00:04:11.960
their engineering departments to ensure that their processes

00:04:11.960 --> 00:04:14.435
protect their client's data and can be easily

00:04:14.435 --> 00:04:17.394
updated to remove personally identifiable information.

00:04:17.394 --> 00:04:22.189
In these cases, we can create tasks in our DAGs that check the data output in

00:04:22.189 --> 00:04:24.589
the stages of our pipelines to ensure

00:04:24.589 --> 00:04:27.500
that sensitive columns in our data are no longer present.

00:04:27.500 --> 00:04:31.019
It's a simple check, but it's an important one.

