WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.410
So another common source where you're going to find data is going to

00:00:05.410 --> 00:00:10.865
be not S3 but an actual Hadoop Distributed File System.

00:00:10.865 --> 00:00:14.200
So we talked about this a little bit before when we were looking

00:00:14.200 --> 00:00:19.210
inside the EMR page for testing that our cluster is up and running,

00:00:19.210 --> 00:00:25.630
but you can actually go to your cluster and as one of the connections,

00:00:25.630 --> 00:00:32.170
you can actually open up the HDFS file system or the Hadoop Distributed File System.

00:00:32.170 --> 00:00:37.930
By going here, you'll get a quick overview of the system and that it's working,

00:00:37.930 --> 00:00:40.960
but the actual place that will be helpful to look at is

00:00:40.960 --> 00:00:44.765
under Utilities and then browse the file system.

00:00:44.765 --> 00:00:49.070
So the Hadoop Distributed File System is just like

00:00:49.070 --> 00:00:54.155
a file system that you'd have on cluster or on a local machine.

00:00:54.155 --> 00:00:59.450
It is distributed but from your perspective it looks just like a normal file system.

00:00:59.450 --> 00:01:04.490
So most of what you would actually want to use is inside this users folder.

00:01:04.490 --> 00:01:09.485
So right now, there's a number of folders that are set up by

00:01:09.485 --> 00:01:13.550
the Amazon EMR and we

00:01:13.550 --> 00:01:17.945
need to actually put some data in here before we can start using Spark with it.

00:01:17.945 --> 00:01:25.205
So let's go ahead and learn how to write files to the Hadoop Distributed File System.

00:01:25.205 --> 00:01:29.690
So to do that what you'll want to do is open up your terminal,

00:01:29.690 --> 00:01:33.480
and if you remember from before,

00:01:33.480 --> 00:01:38.140
we can actually SSH directly into our Spark in my cluster.

00:01:38.140 --> 00:01:42.130
I used the config file to set it up which gives me this nice short command.

00:01:42.130 --> 00:01:47.500
If you didn't use that, you can use the command from the previous concept.

00:01:47.500 --> 00:01:53.060
But when you do that you should be able to log into the EMR cluster.

00:01:53.220 --> 00:01:58.270
What you'll first need to do is copy the actual files that we want to put

00:01:58.270 --> 00:02:02.925
into our distributed file system onto this EMR cluster.

00:02:02.925 --> 00:02:04.690
So to do that,

00:02:04.690 --> 00:02:06.670
I'm going to actually log out again,

00:02:06.670 --> 00:02:16.575
but there's a nice command that does that for you which is simply SCP command.

00:02:16.575 --> 00:02:18.765
So just to review,

00:02:18.765 --> 00:02:21.120
I've actually got some data,

00:02:21.120 --> 00:02:24.420
the data that we want on my desktop.

00:02:24.420 --> 00:02:27.905
Specifically I have those two JSON files

00:02:27.905 --> 00:02:31.985
already stored on my desktop along with some other files.

00:02:31.985 --> 00:02:41.690
So from here, what we want to do is actually copy these two files over to our cluster,

00:02:41.690 --> 00:02:45.020
and we can use the SCP command to do that.

00:02:45.020 --> 00:02:51.005
That's just a copy that uses the Shell or the SSH,

00:02:51.005 --> 00:02:57.395
so a lot like the CP command that you would use normally in Linux.

00:02:57.395 --> 00:03:04.015
Let's copy those two files takes a few seconds for them to go across but pretty quickly.

00:03:04.015 --> 00:03:08.565
Then let's go, ahead and log back into the cluster.

00:03:08.565 --> 00:03:13.850
If we wait for a second and get back into that cluster,

00:03:13.850 --> 00:03:19.020
if we type LS we should be able to see those two files that we just transferred over.

00:03:19.030 --> 00:03:24.655
So now that they're on the actual cluster, the EMR cluster,

00:03:24.655 --> 00:03:29.675
what we need to do is copy them into the HDFS system.

00:03:29.675 --> 00:03:32.075
So right now even though there on that machine,

00:03:32.075 --> 00:03:34.565
they're not actually in this cluster.

00:03:34.565 --> 00:03:38.195
If we hit "Refresh", we still won't see anything here.

00:03:38.195 --> 00:03:42.385
So rather than just copying them into this folder,

00:03:42.385 --> 00:03:46.445
what we first want to do is create a new folder for our data.

00:03:46.445 --> 00:03:51.020
This is just from a perspective of keeping things organized but it's a best practice.

00:03:51.020 --> 00:03:54.380
So what we're going to do is go ahead and

00:03:54.380 --> 00:03:58.700
create a new folder but within the Hadoop Distributed File System.

00:03:58.700 --> 00:04:02.540
So to do that, you have to kind of use this kind of odd language.

00:04:02.540 --> 00:04:05.290
If you were to just try to create a folder locally,

00:04:05.290 --> 00:04:08.090
you might use something like make dir.

00:04:08.090 --> 00:04:13.295
But instead, you have to perceive this command with HDFS DFS.

00:04:13.295 --> 00:04:15.410
It's kind of an odd thing that you have to do it twice

00:04:15.410 --> 00:04:19.905
but for historical reasons it's still there.

00:04:19.905 --> 00:04:22.430
Then you type the command that you would use,

00:04:22.430 --> 00:04:25.625
almost always the same command that you'd use and just Linux,

00:04:25.625 --> 00:04:30.950
but with a hyphen proceeding it and then the name of the folder that we want to create.

00:04:30.950 --> 00:04:33.305
So we're going to put it in that user sub-directory

00:04:33.305 --> 00:04:36.335
and then we'll call it something like sparkify data.

00:04:36.335 --> 00:04:41.300
That's where we're going to keep all our data for this application sparsify.

00:04:41.300 --> 00:04:43.420
So after doing that,

00:04:43.420 --> 00:04:46.910
we should hopefully have everything be successful.

00:04:46.910 --> 00:04:48.470
Just in case you want to know,

00:04:48.470 --> 00:04:50.450
you can always type HDFS by

00:04:50.450 --> 00:04:54.505
itself and they'll give you a list of all the different commands.

00:04:54.505 --> 00:04:58.650
If you want to see the HDFS DFS commands,

00:04:58.650 --> 00:05:00.950
again you'll get a list of all the commands.

00:05:00.950 --> 00:05:04.935
So if you ever forget the syntax you can always just write that and, for example,

00:05:04.935 --> 00:05:09.305
they'll show you exactly the sort of commands that you have available.

00:05:09.305 --> 00:05:14.540
So the command that we actually want to use is one of these ones up here,

00:05:14.540 --> 00:05:17.000
specifically, copy from local.

00:05:17.000 --> 00:05:24.770
So we have this file on the machine and not in the Hadoop Distributed File System,

00:05:24.770 --> 00:05:27.005
it's just on the EMR cluster.

00:05:27.005 --> 00:05:32.420
What we want to do is we want to copy that to the actual HDFS.

00:05:32.420 --> 00:05:34.095
So to do that,

00:05:34.095 --> 00:05:35.720
we'll go and type that command.

00:05:35.720 --> 00:05:40.940
So again, we always precede these commands with HDFS DFS,

00:05:40.940 --> 00:05:44.470
and then we'll do the copy from local.

00:05:44.470 --> 00:05:52.205
Then we're going to specify the name of the file and i'm using autocomplete to do that.

00:05:52.205 --> 00:06:02.585
Let's go ahead and put that in the user sparkify data sub-directory that we just created.

00:06:02.585 --> 00:06:04.580
So we'll go ahead and do this,

00:06:04.580 --> 00:06:08.555
and we'll go ahead and return back to

00:06:08.555 --> 00:06:14.090
our browser to see that HDFS UI and go ahead and refresh,

00:06:14.090 --> 00:06:18.635
and now we'll see not only is there this folder that we created,

00:06:18.635 --> 00:06:23.240
but within that we can see the sparkify log file that we just made.

00:06:23.240 --> 00:06:25.205
So right now we just have that one.

00:06:25.205 --> 00:06:28.310
Let's go ahead and copy over that second file as well.

00:06:28.310 --> 00:06:32.455
It's just the same thing but with two attached to it.

00:06:32.455 --> 00:06:37.055
Again, let's double check that it actually went there and there it is.

00:06:37.055 --> 00:06:39.230
So you could actually inspect it,

00:06:39.230 --> 00:06:41.960
you could look at some more properties for it,

00:06:41.960 --> 00:06:46.415
but for our sake it's just good to see that it's there.

00:06:46.415 --> 00:06:50.210
So once you have the file actually on HDFS,

00:06:50.210 --> 00:06:56.855
let's return back to our spark log book and let's actually try to read data from that.

00:06:56.855 --> 00:06:59.740
So let's create a new cell.

00:06:59.740 --> 00:07:05.840
Again, you can find some of these commands here but it's very similar you would

00:07:05.840 --> 00:07:11.825
just almost exactly the same as our previous command to read it from S3.

00:07:11.825 --> 00:07:17.180
The key difference being here is that you instead of putting S3 n in front,

00:07:17.180 --> 00:07:19.940
you actually specify HDFS,

00:07:19.940 --> 00:07:22.700
and then it does look like an actual file path.

00:07:22.700 --> 00:07:27.080
Now I will note that there's one kind of confusing thing here which is much like

00:07:27.080 --> 00:07:31.625
you'd type HTTP:// to get a URL.

00:07:31.625 --> 00:07:33.950
You do the same thing here for HDFS.

00:07:33.950 --> 00:07:39.095
However, because the actual file and if we go to this from here,

00:07:39.095 --> 00:07:46.180
the actual file that you're getting this from is /user.

00:07:46.180 --> 00:07:48.000
What this means is that,

00:07:48.000 --> 00:07:50.450
you actually have the same /user here.

00:07:50.450 --> 00:07:53.240
So you weirdly have these three slashes

00:07:53.240 --> 00:07:56.030
in a row and that quite often is a place where you'll

00:07:56.030 --> 00:08:01.920
see errors is that people will only forget this and only put one or two.

00:08:01.920 --> 00:08:06.950
So if you do this and you actually try loading the same data,

00:08:06.950 --> 00:08:09.530
you will see everything seems to happen.

00:08:09.530 --> 00:08:14.720
Spark again now pulls the data but this time it pulls it from HDFS,

00:08:14.720 --> 00:08:20.540
and we can run the same exact command as below.

00:08:20.540 --> 00:08:23.975
Let's go in and actually look at this data,

00:08:23.975 --> 00:08:29.670
and let's go ahead and read like the first three lines of it.

00:08:30.610 --> 00:08:35.269
We'll go ahead and see the data is pulling as we expected,

00:08:35.269 --> 00:08:38.900
and we can actually see all of this status.

00:08:38.900 --> 00:08:43.130
So looks good so far everything's working as expected,

00:08:43.130 --> 00:08:52.860
and this is how you pull data from both HDFS S3 and from the files themselves.

