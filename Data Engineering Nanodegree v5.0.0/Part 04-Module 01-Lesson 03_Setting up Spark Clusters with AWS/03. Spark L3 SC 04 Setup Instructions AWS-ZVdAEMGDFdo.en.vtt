WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.280
To fully leverage the power of Spark,

00:00:02.280 --> 00:00:04.860
you'll need to go beyond your local computer and run

00:00:04.860 --> 00:00:07.920
a distributed cluster made up of multiple machines.

00:00:07.920 --> 00:00:10.620
This is usually done by renting computers from

00:00:10.620 --> 00:00:15.420
a cloud provider like Amazon Web Services or AWS for short.

00:00:15.420 --> 00:00:17.670
Spinning up multiple machines,

00:00:17.670 --> 00:00:21.920
then downloading and installing Spark and all of its dependencies on each of them,

00:00:21.920 --> 00:00:25.725
and finally configuring them to work together can be a bit tricky.

00:00:25.725 --> 00:00:28.140
If you'd like to see how to do this manually,

00:00:28.140 --> 00:00:31.335
we've included a guide to give you a sense of the process.

00:00:31.335 --> 00:00:34.740
If you have to do this by hand for several machines,

00:00:34.740 --> 00:00:36.510
you probably make a few mistakes,

00:00:36.510 --> 00:00:39.840
and even if you don't, you'll probably find it very tedious.

00:00:39.840 --> 00:00:42.190
Plus if you want to update anything,

00:00:42.190 --> 00:00:44.210
you'll have to do it several times.

00:00:44.210 --> 00:00:51.460
Fortunately, AWS offers an easier option called Elastic MapReduce or EMR for short.

00:00:51.460 --> 00:00:55.400
EMR is a service that provides you EC2 instances with

00:00:55.400 --> 00:01:00.410
many big data technologies like Hadoop and Spark already installed and configured.

00:01:00.410 --> 00:01:06.530
Start off by creating an AWS account if you haven't already and login to the AWS console.

00:01:06.530 --> 00:01:08.560
Before we start using the EMR,

00:01:08.560 --> 00:01:14.050
we'll need to create an SSH key pair to securely connect to the cluster we'll create.

00:01:14.050 --> 00:01:19.320
To do this, go to the EC2 menu which you'll find under Services,

00:01:21.580 --> 00:01:25.355
and then find key pairs on the left.

00:01:25.355 --> 00:01:27.920
You may already have some key pairs,

00:01:27.920 --> 00:01:33.125
but let's create a new key pair for our Spark cluster and give it an intuitive name.

00:01:33.125 --> 00:01:37.990
A pem file should automatically download onto your local machine.

00:01:37.990 --> 00:01:40.625
For me, it's in my downloads folder.

00:01:40.625 --> 00:01:42.730
Make sure you know where this key file is,

00:01:42.730 --> 00:01:44.660
you'll need it in a few minutes.

00:01:44.660 --> 00:01:49.445
This is one half of the key pair and Amazon will put the other on the cluster.

00:01:49.445 --> 00:01:54.230
Only when you have both keys together will you be able to connect to the machine.

00:01:54.230 --> 00:01:58.950
Next, we'll go to the EMR Service.

00:01:58.950 --> 00:02:04.105
Give your cluster name and enable logging so we can track any errors.

00:02:04.105 --> 00:02:06.580
Keep the default S3 location,

00:02:06.580 --> 00:02:08.725
which we'll talk about later in this lesson.

00:02:08.725 --> 00:02:13.570
Make sure you use the cluster setting which will create a long-term cluster for you.

00:02:13.570 --> 00:02:16.510
The step execution option will automatically

00:02:16.510 --> 00:02:19.765
tear down your cluster once your Spark job finishes.

00:02:19.765 --> 00:02:24.115
That's cost-effective if you've already perfected your Spark job for your data.

00:02:24.115 --> 00:02:26.440
But since we'll be exploring and developing,

00:02:26.440 --> 00:02:28.460
we'll use a long-term cluster.

00:02:28.460 --> 00:02:36.550
Select EMR version 5.20.0 and make sure to select the option with Spark 2.4.0 included.

00:02:36.550 --> 00:02:42.185
This comes with the Hadoop Distributed File System or HDFS for short already installed.

00:02:42.185 --> 00:02:44.450
We'll discuss this later in this lesson.

00:02:44.450 --> 00:02:47.525
EMR operates spark in YARN cluster mode,

00:02:47.525 --> 00:02:50.920
which has a few small differences from the simpler Standalone mode.

00:02:50.920 --> 00:02:53.480
But also offers a few extra features.

00:02:53.480 --> 00:02:56.900
This EMR version also comes with a monitoring tool

00:02:56.900 --> 00:03:00.125
called Ganglia and Zeppelin for Scala notebooks.

00:03:00.125 --> 00:03:03.155
But we won't use either of these in this course.

00:03:03.155 --> 00:03:06.785
For this course, we'll use M5 extra larges.

00:03:06.785 --> 00:03:10.505
Let's quickly go over the instance types and AWS.

00:03:10.505 --> 00:03:13.340
The M indicates the multipurpose family,

00:03:13.340 --> 00:03:15.860
as opposed to the R family that is optimized for

00:03:15.860 --> 00:03:19.585
RAM or the C family that is optimized for CPU.

00:03:19.585 --> 00:03:23.560
There's a family for each of the major components we discussed in lesson 1,

00:03:23.560 --> 00:03:27.295
so you can optimize for your use case once you understand it.

00:03:27.295 --> 00:03:31.585
However, it's generally best to start with the multipurpose M family.

00:03:31.585 --> 00:03:33.890
So we'll use M for this cluster.

00:03:33.890 --> 00:03:37.375
With EMR, it's always easier to change this later.

00:03:37.375 --> 00:03:40.480
The five indicates the fifth generation of hardware from

00:03:40.480 --> 00:03:44.920
Amazon which is usually cheaper and more powerful than previous generations.

00:03:44.920 --> 00:03:49.225
The fifth generation automatically comes with SSD storage.

00:03:49.225 --> 00:03:51.775
Lastly, the size of the instance,

00:03:51.775 --> 00:03:53.320
in this case extra large,

00:03:53.320 --> 00:03:56.050
indicates the overall hardware quality.

00:03:56.050 --> 00:03:58.645
A size of small will have less CPU,

00:03:58.645 --> 00:04:01.560
memory, storage, and networking than an extra large,

00:04:01.560 --> 00:04:06.995
but the rough ratios between the hardware components stay the same within a given family.

00:04:06.995 --> 00:04:13.085
Instances can go as low as nano and as high as eight extra large for some families.

00:04:13.085 --> 00:04:15.950
You can check out the resources in the classroom to learn

00:04:15.950 --> 00:04:21.170
the specific hardware specifications and brands that these instances correspond to.

00:04:21.170 --> 00:04:25.250
EMR refers to the worker nodes as core nodes since they'll store

00:04:25.250 --> 00:04:29.540
data on HDFS as well as doing work on the data.

00:04:29.540 --> 00:04:32.740
Pricing for these nodes is about $0.05 per hour,

00:04:32.740 --> 00:04:35.885
so the whole cluster will be roughly $0.20 per hour.

00:04:35.885 --> 00:04:39.815
This is cheap enough that we can learn a lot on a reasonable budget.

00:04:39.815 --> 00:04:44.150
Also with EMR, you can easily turn off your cluster when you're not using it.

00:04:44.150 --> 00:04:47.140
So you only pay for them when you need them.

00:04:47.140 --> 00:04:50.450
EMR has a few small costs that come with it like

00:04:50.450 --> 00:04:53.330
S3 storage for your logs and Jupyter notebooks.

00:04:53.330 --> 00:04:55.880
But it's well worth these small extra charges.

00:04:55.880 --> 00:04:59.360
If you want to learn more about selecting the right machines to rent,

00:04:59.360 --> 00:05:02.525
there are details included and links in the classroom.

00:05:02.525 --> 00:05:05.135
But most people will stick with the defaults.

00:05:05.135 --> 00:05:08.280
Make sure you pick the SSH key you made earlier,

00:05:08.280 --> 00:05:11.510
you'll need to do this to be able to connect to your cluster.

00:05:11.510 --> 00:05:15.420
You can leave all the other options on the defaults.

