WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.600
All right. So now that we've seen how to run our Spark code and a Jupiter Notebook,

00:00:06.600 --> 00:00:13.080
the next thing that we want to do is look at how to submit a standalone application,

00:00:13.080 --> 00:00:16.080
and there's lots of different reasons why you'd want to do this.

00:00:16.080 --> 00:00:22.035
One of the most useful things about the Jupiter notebooks is that they're interactive,

00:00:22.035 --> 00:00:24.360
which makes them easy to share with colleagues,

00:00:24.360 --> 00:00:30.015
that makes it easy to share visualizations or exploration data.

00:00:30.015 --> 00:00:31.845
But when it comes down to it,

00:00:31.845 --> 00:00:35.300
you want this Spark job to be deployed typically in

00:00:35.300 --> 00:00:37.160
a production server where you don't

00:00:37.160 --> 00:00:39.830
necessarily want it to be running on a Jupiter notebook.

00:00:39.830 --> 00:00:45.845
Instead, you want it to run on a script that will stand alone in a terminal environment.

00:00:45.845 --> 00:00:49.820
So it's very important to be able to deploy whatever code that you

00:00:49.820 --> 00:00:54.200
have in a Jupiter notebook also in a stand-alone Python script.

00:00:54.200 --> 00:00:58.085
So let's go ahead and go through the exercise of converting

00:00:58.085 --> 00:01:02.675
this notebook into a stand-alone Python script that you submit.

00:01:02.675 --> 00:01:06.725
There's a few gotchas that you'll find along the way.

00:01:06.725 --> 00:01:08.765
So let's return to our terminal.

00:01:08.765 --> 00:01:12.170
You should be logged in to the Hadoop EMR.

00:01:12.170 --> 00:01:17.690
If you're not, just make sure you are logged in and if you've got logged out,

00:01:17.690 --> 00:01:20.060
just hit that SSH command to get logged in.

00:01:20.060 --> 00:01:22.610
We have the data that we've imported before,

00:01:22.610 --> 00:01:26.015
and we're just going to again use that dummy song data.

00:01:26.015 --> 00:01:30.650
So I went ahead and made a Python file with all of

00:01:30.650 --> 00:01:35.975
the code that we had from the Jupiter notebook with a few modifications.

00:01:35.975 --> 00:01:41.640
In order to see those, go ahead and open your editor of choice.

00:01:41.640 --> 00:01:43.310
For me, nano is a pretty simple one.

00:01:43.310 --> 00:01:44.960
So I'm going to go ahead and do this.

00:01:44.960 --> 00:01:47.780
You could also create this file locally on

00:01:47.780 --> 00:01:51.940
your computer using whatever editor you prefer,

00:01:51.940 --> 00:01:56.690
and then copy the file over and I will show you how to do that in a little bit as well.

00:01:56.690 --> 00:01:58.670
But for the time being, let's go ahead and take a look at

00:01:58.670 --> 00:02:01.820
this Python file called lower_songs.

00:02:01.820 --> 00:02:03.680
So it's pretty simple file.

00:02:03.680 --> 00:02:06.510
Again, it's most of the same code that we saw before,

00:02:06.510 --> 00:02:08.450
but with a few key differences.

00:02:08.450 --> 00:02:09.670
So first of all,

00:02:09.670 --> 00:02:16.560
something that you'll see is that we have the from pyspark.sql import SparkSession,

00:02:16.560 --> 00:02:21.125
and this is imported just as if it was any old Python module.

00:02:21.125 --> 00:02:25.490
So this is something that's already pre-installed on the EMR cluster for you.

00:02:25.490 --> 00:02:28.270
You don't have to do any pip or anything like that to install it,

00:02:28.270 --> 00:02:30.620
but you do have to remember to import it.

00:02:30.620 --> 00:02:33.410
As a comparison, you'll notice here we did not

00:02:33.410 --> 00:02:35.935
have to do that when we were using the Jupiter notebook.

00:02:35.935 --> 00:02:39.530
That's because the Jupiter notebook assumes that you're already

00:02:39.530 --> 00:02:43.100
running this and imports this for you automatically.

00:02:43.100 --> 00:02:45.395
So you don't have to do this in Jupiter notebook,

00:02:45.395 --> 00:02:48.395
but you do need to do this when you have a standalone script.

00:02:48.395 --> 00:02:51.350
The other thing that you'll see here that is a bit novel.

00:02:51.350 --> 00:02:55.040
So of course, there's a bit of standard Python.

00:02:55.040 --> 00:02:57.650
So specifying that this is going to

00:02:57.650 --> 00:03:00.790
be effectively the main function so that it can be run as a module.

00:03:00.790 --> 00:03:04.310
But the other non-trivial thing that's specific to Spark

00:03:04.310 --> 00:03:08.600
is the fact that we have this spark equals SparkSession.

00:03:08.600 --> 00:03:12.080
So this is where we actually instantiate

00:03:12.080 --> 00:03:18.005
the SparkSession module that we're using and specifically, it has this format.

00:03:18.005 --> 00:03:22.580
It's usually written in this multi-line format so that you can have it stretch on.

00:03:22.580 --> 00:03:24.815
It's potentially has many different options.

00:03:24.815 --> 00:03:27.890
But it's going to be SparkSession, and then builder.

00:03:27.890 --> 00:03:29.510
Those are standard things,

00:03:29.510 --> 00:03:31.715
and then you have the application name.

00:03:31.715 --> 00:03:34.940
So this is what you're going to see when you actually submit

00:03:34.940 --> 00:03:38.465
your script on the EMR Spark UI.

00:03:38.465 --> 00:03:40.505
So I just chose something intuitive.

00:03:40.505 --> 00:03:43.310
Lower song titles, that is all this script does.

00:03:43.310 --> 00:03:46.850
Then importantly, use get or create.

00:03:46.850 --> 00:03:52.460
What that will do is it will get the SparkSession if it already exists.

00:03:52.460 --> 00:03:55.800
So that way, it doesn't accidentally create too.

00:03:55.800 --> 00:03:58.250
But if it doesn't exist, it will create a brand new one.

00:03:58.250 --> 00:04:00.050
So it's a nice feature there.

00:04:00.050 --> 00:04:05.765
So that's something new that wasn't present in the previous examples here.

00:04:05.765 --> 00:04:11.045
Everything else is more or less exactly what you saw with one small exception.

00:04:11.045 --> 00:04:15.470
One of the things you'll see in the Python Jupiter notebooks is that,

00:04:15.470 --> 00:04:19.205
we had a special thing called SC.

00:04:19.205 --> 00:04:22.055
If you remember that was actually the Spark context,

00:04:22.055 --> 00:04:24.050
and it's actually the same thing here.

00:04:24.050 --> 00:04:26.540
Just for the Python Jupiter notebooks,

00:04:26.540 --> 00:04:28.430
it's abbreviated as SC.

00:04:28.430 --> 00:04:31.710
Unless you create that abbreviation as well,

00:04:31.710 --> 00:04:35.780
you'll have to actually write out the full spark.spark context.

00:04:35.780 --> 00:04:42.620
Again, that's coming from the SparkSession module and then created by using this spark.

00:04:42.620 --> 00:04:46.535
So this is really the unique things that you have to add.

00:04:46.535 --> 00:04:49.910
Other than that, it's the standard print function that we had before.

00:04:49.910 --> 00:04:53.465
Then finally, the last thing that's novel about the Spark script

00:04:53.465 --> 00:04:57.360
is that you do have to explicitly tell the Spark's script to stop.

00:04:57.360 --> 00:05:01.535
Otherwise, the application would just hang and sometimes you want that.

00:05:01.535 --> 00:05:06.040
For example, in the case of a Spark streaming job that you want running for a long time.

00:05:06.040 --> 00:05:08.420
But for the case of a batch job where you have

00:05:08.420 --> 00:05:12.455
some finite set of data that you want to process once and just once,

00:05:12.455 --> 00:05:16.540
you'll go ahead and do a Spark stop at the very end of your script.

00:05:16.540 --> 00:05:19.115
Okay. So that's the script.

00:05:19.115 --> 00:05:20.900
Let's go ahead and leave this,

00:05:20.900 --> 00:05:25.085
and then see how to actually submit the script to the Spark cluster.

00:05:25.085 --> 00:05:31.295
So the command to do that is actually called spark submit pretty intuitive.

00:05:31.295 --> 00:05:35.795
However, it's not obvious where that command actually is located.

00:05:35.795 --> 00:05:39.605
It's in a slightly different location for every system.

00:05:39.605 --> 00:05:42.455
Sometimes it's created in a spark home,

00:05:42.455 --> 00:05:46.850
but on EMR, it's actually in the following place user bin.

00:05:46.850 --> 00:05:48.470
If you ever forget where,

00:05:48.470 --> 00:05:52.250
there's always a nice Linux command that will help you find that called which.

00:05:52.250 --> 00:05:54.590
So if you ever forget where this is,

00:05:54.590 --> 00:05:57.540
you can always type which spark-submit,

00:05:57.540 --> 00:06:00.425
and it will actually show you the full path for that.

00:06:00.425 --> 00:06:03.065
So if you want, you can just copy that command.

00:06:03.065 --> 00:06:06.350
Then the syntax has a lot of different options.

00:06:06.350 --> 00:06:09.680
But generally speaking, most of them are

00:06:09.680 --> 00:06:14.165
just the name of the particular option you want preceded by two hyphens.

00:06:14.165 --> 00:06:20.135
So for example, you typically have to specify the location of the master node.

00:06:20.135 --> 00:06:24.965
Sometimes this is an IP address if you were using other Spark deployments.

00:06:24.965 --> 00:06:27.605
But because we're using the Spark yarn mode,

00:06:27.605 --> 00:06:29.480
we simply type yarn here.

00:06:29.480 --> 00:06:33.860
It's really simple. Yarn is a resource negotiator or resource manager,

00:06:33.860 --> 00:06:37.685
and so it will be able to automatically detect the IP.

00:06:37.685 --> 00:06:40.820
You don't have to enter it yourself which is a nice convenience.

00:06:40.820 --> 00:06:43.670
The next thing that you have to do is well,

00:06:43.670 --> 00:06:46.820
you could mark number just think configurations.

00:06:46.820 --> 00:06:48.480
But at a bare minimum,

00:06:48.480 --> 00:06:53.615
you would just put the path of the actual Python script that you'd like to run.

00:06:53.615 --> 00:06:55.760
So in this case, it's in the current directory.

00:06:55.760 --> 00:06:58.910
So we'll just go ahead and type that out.

00:06:58.910 --> 00:07:00.920
Then once you hit enter,

00:07:00.920 --> 00:07:04.310
it should submit that job to the spark cluster.

00:07:04.310 --> 00:07:07.850
So you're going to see a lot of these info.

00:07:07.850 --> 00:07:09.860
So I'll kind of pause it for a second.

00:07:09.860 --> 00:07:13.100
You're going to see a lot of things that say info on it.

00:07:13.100 --> 00:07:17.465
If it says info, it's just really telling you information.

00:07:17.465 --> 00:07:19.310
It's not necessarily a problem.

00:07:19.310 --> 00:07:22.220
You'll also see a few of these that say WARN.

00:07:22.220 --> 00:07:23.480
In this particular case,

00:07:23.480 --> 00:07:29.450
this is a warning about the Spark UI not being able to use port 4040.

00:07:29.450 --> 00:07:33.110
That's perfectly okay because we're actually already running something there.

00:07:33.110 --> 00:07:36.700
So that's not a problem per se because on the next line,

00:07:36.700 --> 00:07:39.995
you can see that it was able to run it on 4041.

00:07:39.995 --> 00:07:45.530
So you're going to see a lot of things whenever you submit these applications.

00:07:45.530 --> 00:07:49.310
Most of them, you can more or less ignore and most people learn how to.

00:07:49.310 --> 00:07:52.460
We can also show you in a later video how

00:07:52.460 --> 00:07:57.540
to turn off down these warnings so that you only see for example,

00:07:57.540 --> 00:08:00.560
don't see the info's, you only see the warnings or errors.

00:08:00.560 --> 00:08:03.980
But most people just leave them on and go from there.

00:08:03.980 --> 00:08:07.350
So occasionally, you'll see a bigger block like this,

00:08:07.350 --> 00:08:10.040
and this can be quite useful because it contains

00:08:10.040 --> 00:08:14.135
the URL where you can actually find more information.

00:08:14.135 --> 00:08:16.910
So if you click on this or alternatively,

00:08:16.910 --> 00:08:20.105
if you go back to our Spark UI,

00:08:20.105 --> 00:08:23.240
what you'll see if you refresh the page is that there

00:08:23.240 --> 00:08:27.480
is a new application that was ran at the very top.

00:08:27.480 --> 00:08:29.685
So if you actually click on that application,

00:08:29.685 --> 00:08:33.155
you can learn all sorts of information about the Spark UI.

00:08:33.155 --> 00:08:36.965
We'll be going through all the details and how to use this to debug.

00:08:36.965 --> 00:08:40.150
But effectively, you can see that in this case,

00:08:40.150 --> 00:08:42.510
the job ran successfully,

00:08:42.510 --> 00:08:45.380
and you can also click even further down.

00:08:45.380 --> 00:08:47.630
You can look at individual steps.

00:08:47.630 --> 00:08:49.565
You can look at the timeline.

00:08:49.565 --> 00:08:51.965
You can visualize the DAG.

00:08:51.965 --> 00:08:56.690
Most importantly, you can also see the standard out and standard errors if there

00:08:56.690 --> 00:09:01.580
were any for particular processes and particular tasks.

00:09:01.580 --> 00:09:02.960
So in this case,

00:09:02.960 --> 00:09:05.000
those wouldn't be interesting.

00:09:05.000 --> 00:09:06.545
There's nothing really that went wrong.

00:09:06.545 --> 00:09:11.000
But this is a useful thing that will go over in more detail later.

00:09:11.000 --> 00:09:12.770
Returning to our script,

00:09:12.770 --> 00:09:15.545
let's go ahead and actually see the thing of note,

00:09:15.545 --> 00:09:17.375
the result that we're looking for.

00:09:17.375 --> 00:09:22.460
So one of the challenges with running a Python script as

00:09:22.460 --> 00:09:27.995
opposed to a Jupiter notebook is that it is a little bit hard to actually see the output.

00:09:27.995 --> 00:09:29.360
If you look carefully,

00:09:29.360 --> 00:09:33.350
you'll notice that this line does actually have the output,

00:09:33.350 --> 00:09:36.695
but it's buried in the middle of the process.

00:09:36.695 --> 00:09:38.960
It's sometimes not clear,

00:09:38.960 --> 00:09:42.840
especially if your output is just a simple number or something like that.

00:09:42.840 --> 00:09:45.500
It can be very easy to miss that.

00:09:45.500 --> 00:09:50.120
So one of the things that you'll actually want to do is rather than just printing out to

00:09:50.120 --> 00:09:52.520
the command line which it's useful

00:09:52.520 --> 00:09:55.090
when you're developing code and making sure that things are working.

00:09:55.090 --> 00:09:57.920
It's a much better practice to actually write the output of

00:09:57.920 --> 00:10:01.940
your file to an actual output file.

00:10:01.940 --> 00:10:04.385
Instead of writing it just to the command line.

00:10:04.385 --> 00:10:07.025
That also makes it easy to see the result.

00:10:07.025 --> 00:10:08.540
If I were to close this terminal,

00:10:08.540 --> 00:10:12.360
I'd lose the results and then have to run the Spark job,

00:10:12.360 --> 00:10:14.350
which in this case took just a few seconds,

00:10:14.350 --> 00:10:16.085
but it could have taken a lot longer.

00:10:16.085 --> 00:10:18.020
You wouldn't want that to be the case.

00:10:18.020 --> 00:10:21.560
So the common best practice is to actually write to a file,

00:10:21.560 --> 00:10:24.315
and we'll show how to do that in a second. But that's it.

00:10:24.315 --> 00:10:28.490
How you submit Python application and in this case,

00:10:28.490 --> 00:10:32.560
a simple application to just lowercase the words of songs.

00:10:32.560 --> 00:10:35.195
But we'll go through some more complicated ones.

00:10:35.195 --> 00:10:37.490
It's always very important to make sure

00:10:37.490 --> 00:10:40.190
that you can get a very simple application to work,

00:10:40.190 --> 00:10:43.940
and I suggest you do that yourself before trying to do

00:10:43.940 --> 00:10:48.535
anything more complicated like loading input data from a different source,

00:10:48.535 --> 00:10:52.970
or trying to do some more complicated calculations.

