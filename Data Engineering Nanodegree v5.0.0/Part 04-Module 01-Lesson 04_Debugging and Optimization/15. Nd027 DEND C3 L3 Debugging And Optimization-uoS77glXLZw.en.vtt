WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.085
So in this video,

00:00:02.085 --> 00:00:07.665
we're going to go over what the skewed dataset would look like.

00:00:07.665 --> 00:00:11.970
I'm going to use the pandas library in Python because it

00:00:11.970 --> 00:00:16.170
gives me the freedom of exploring a CSV file pretty quickly.

00:00:16.170 --> 00:00:22.110
So I'm going to do import pandas as pd.

00:00:23.450 --> 00:00:34.060
I'm going to read my CSV file that is currently under my folders in Jupyter directory.

00:00:34.060 --> 00:00:36.585
I want to do df.columns.

00:00:36.585 --> 00:00:39.120
This list gives me all the column names.

00:00:39.120 --> 00:00:47.625
So let's say if your DataFrame or the CSV file had long columns,

00:00:47.625 --> 00:00:49.425
many numbers of columns,

00:00:49.425 --> 00:00:55.744
then you'll be able to see the column names without showing all the DataFrame.

00:00:55.744 --> 00:00:58.530
So let's execute.

00:01:03.590 --> 00:01:06.485
You'll see this warning sign,

00:01:06.485 --> 00:01:09.270
but you can ignore it.

00:01:12.020 --> 00:01:15.900
So you can see all the column names,

00:01:15.900 --> 00:01:21.110
and particularly because I already know what the dataset looks like,

00:01:21.230 --> 00:01:26.340
there is months and year column.

00:01:26.340 --> 00:01:30.570
Hypothetically, the Summons_Number, Issue_Date,

00:01:30.570 --> 00:01:36.020
the Plate_ID will be fairly unique because not many people will

00:01:36.020 --> 00:01:42.735
get to get more than two or three times, hopefully, and Registration_State.

00:01:42.735 --> 00:01:47.215
Since this is NYC parking violation data,

00:01:47.215 --> 00:01:51.060
I'm assuming there will be a bunch of New York State.

00:01:51.410 --> 00:01:56.015
Then there will be a bunch of Issue_Date, Violation_Code, yes, maybe.

00:01:56.015 --> 00:02:00.620
But we want to focus on year and month for now just

00:02:00.620 --> 00:02:05.705
to see how skewed this data is if you were to process over a year and month.

00:02:05.705 --> 00:02:11.970
What you can do is you can df.year.value_counts.

00:02:12.590 --> 00:02:18.535
You can see this dataset is heavily focused on 2015 and 2014.

00:02:18.535 --> 00:02:24.270
So let's do month.value_counts.

00:02:25.670 --> 00:02:29.070
This one is more evenly distributed.

00:02:29.070 --> 00:02:38.540
You can see the difference is really about like 80,000 tickets,

00:02:38.540 --> 00:02:41.210
which is not that much for this dataset.

00:02:41.210 --> 00:02:45.240
Let's take a look at the registration state as well.

00:02:53.460 --> 00:02:56.280
Here you can see again,

00:02:56.280 --> 00:02:59.935
that New York has the highest value here,

00:02:59.935 --> 00:03:02.260
and then New Jersey, and the rest of the state.

00:03:02.260 --> 00:03:04.870
So let's say if you were to process

00:03:04.870 --> 00:03:12.430
your Spark program based on the year or maybe the state column,

00:03:12.430 --> 00:03:17.650
then your process for your partition that

00:03:17.650 --> 00:03:22.690
is processing over these two years or these two states,

00:03:22.690 --> 00:03:23.950
or you don't even have to go New Jersey,

00:03:23.950 --> 00:03:25.270
it's just the New York State,

00:03:25.270 --> 00:03:30.480
this process will take much longer to process.

00:03:30.480 --> 00:03:36.860
For example, let's say we have like 10 workers and they're not evenly distributed,

00:03:36.860 --> 00:03:40.730
and then one worker is just processing 2015 record,

00:03:40.730 --> 00:03:45.695
and then another one is working on 214, and then the rest,

00:03:45.695 --> 00:03:49.340
the eight workers are focusing on processing these,

00:03:49.340 --> 00:03:55.880
then obviously those eight workers will be

00:03:55.880 --> 00:04:03.750
finished faster than these two because this will take way longer to process. That's it.

