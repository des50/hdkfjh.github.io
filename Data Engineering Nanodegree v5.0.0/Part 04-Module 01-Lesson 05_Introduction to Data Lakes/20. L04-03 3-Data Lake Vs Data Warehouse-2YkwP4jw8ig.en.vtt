WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.169
This is to sum up is a table

00:00:04.169 --> 00:00:08.129
doing the comparison between the data lake versus the data warehouse.

00:00:08.130 --> 00:00:11.580
In data warehouse we deal with tabular formats,

00:00:11.580 --> 00:00:14.070
in data lake we deal with all formats.

00:00:14.070 --> 00:00:22.140
In the data warehouse we have only high value data,

00:00:22.140 --> 00:00:23.850
in the data lake we have high value,

00:00:23.850 --> 00:00:26.760
medium value, and actually to be discovered value.

00:00:26.760 --> 00:00:31.109
So we can put it and see whether this data is valuable or not.

00:00:31.109 --> 00:00:33.149
Then we have for the ingestion mode,

00:00:33.149 --> 00:00:38.774
the ETL extract transform load versus ELT extract load, and then transform.

00:00:38.774 --> 00:00:41.629
The data model in the warehouse is very well defined.

00:00:41.630 --> 00:00:46.740
We have the star and the snowflake schemas with conformed dimensions or with data-marts,

00:00:46.740 --> 00:00:49.560
the Inmon model, the Kimball model,

00:00:49.560 --> 00:00:53.200
the Hybrid model and at the end of the day,

00:00:53.200 --> 00:00:55.010
when we present things to the users,

00:00:55.009 --> 00:00:57.655
we pre-aggregate those OLAP cubes,

00:00:57.655 --> 00:01:01.035
whether OR OLAP, or M OLAP, and so forth.

00:01:01.034 --> 00:01:05.030
In the data lake, we can still serve the same thing.

00:01:05.030 --> 00:01:08.329
No one prevents us from doing star and snowflakes and

00:01:08.329 --> 00:01:12.254
OLAP and and serve them using the current big data tools.

00:01:12.254 --> 00:01:16.174
However, there are other ad-hoc representations.

00:01:16.174 --> 00:01:24.329
For machine learning, you might want to express things differently in different formats.

00:01:25.750 --> 00:01:30.560
The schema is known beforehand in the data warehouse.

00:01:30.560 --> 00:01:34.564
It's on-the-fly at the time of analysis schema-on-read in the data lake.

00:01:34.564 --> 00:01:37.310
The data warehouse you have

00:01:37.310 --> 00:01:40.085
the expensive MPP databases and

00:01:40.084 --> 00:01:43.339
with expensive hardware and connectivity and discs and so forth.

00:01:43.340 --> 00:01:46.820
On data lake you have commodity hardware with parallelism asked

00:01:46.819 --> 00:01:52.744
first principle and also with storage and data in the same place.

00:01:52.745 --> 00:01:55.395
For the data warehouse that ticket quality,

00:01:55.394 --> 00:01:59.310
you have high-quality with lots

00:01:59.310 --> 00:02:03.650
of efforts for consistency and clear rules of accessibility.

00:02:03.650 --> 00:02:08.390
While the data warehouse puts some rules also for the ability

00:02:08.389 --> 00:02:10.609
to change which is the point of dimensions that

00:02:10.610 --> 00:02:13.070
you can actually modify the dimensions and so on,

00:02:13.069 --> 00:02:17.334
the limits of the change are within the

00:02:17.335 --> 00:02:23.930
tabular and the whole mental model of the star schema and the dimensions.

00:02:23.930 --> 00:02:26.790
In the data lake, no. I mean it's mixed.

00:02:26.789 --> 00:02:30.889
Some are a tabular some are in row format

00:02:30.889 --> 00:02:35.389
and some has been curated

00:02:35.389 --> 00:02:40.474
and became of high-quality while other remains still unattended.

00:02:40.474 --> 00:02:47.974
In the warehouse, you have your business analysts as the main customer or the main user.

00:02:47.974 --> 00:02:50.060
In data lake, you have the data scientists,

00:02:50.060 --> 00:02:54.155
the business analyst and new roles like the ML engineers

00:02:54.155 --> 00:02:56.360
who want to know how to get the data

00:02:56.360 --> 00:02:59.650
the best way for the machine running problems for instance.

00:02:59.650 --> 00:03:01.379
Finally, in the data warehouse,

00:03:01.379 --> 00:03:04.394
you have the reports and the business intelligence visualizations,

00:03:04.395 --> 00:03:07.290
while in the data lake you still have the reports,

00:03:07.289 --> 00:03:08.750
the business intelligent visualizations,

00:03:08.750 --> 00:03:10.099
but you have also

00:03:10.099 --> 00:03:17.340
the new advanced analytics like ML graphs and data exploration in general.

00:03:17.340 --> 00:03:21.675
It's perhaps due to say where the name came from.

00:03:21.675 --> 00:03:25.660
There is this famous analogy about the data lake.

00:03:25.659 --> 00:03:30.439
The data warehouse is like a producer of water where you go

00:03:30.439 --> 00:03:34.865
and you're handed bottled water in a particular size and shape of the Bottle.

00:03:34.865 --> 00:03:37.030
So it has to be this way,

00:03:37.030 --> 00:03:38.729
which is actually clean,

00:03:38.729 --> 00:03:44.219
it's distilled, it's beautiful and with high-quality and we know that.

00:03:44.219 --> 00:03:47.460
In contrast, a data lake is like a lake,

00:03:47.460 --> 00:03:56.219
and a lake usually is formed by many streams coming from the mountains flowing into it.

00:03:56.219 --> 00:03:59.439
But the idea is all the water is there,

00:03:59.439 --> 00:04:05.199
and it's up to everyone to go and see how he wants to get water out of this.

00:04:05.199 --> 00:04:10.159
Thus the name of actually the data lake.

