WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:08.470
In this demo, I'm going to show you actually how to access S3 from Spark.

00:00:08.470 --> 00:00:13.179
The idea also would be that the first thing I need to do is

00:00:13.179 --> 00:00:17.844
actually to have a configuration file with an access key and secret key,

00:00:17.844 --> 00:00:25.184
and we have shown this I believe all the way from Lesson 3,

00:00:25.184 --> 00:00:30.149
how to get the access key and a secret access key,

00:00:30.149 --> 00:00:32.379
and we put them in a config file.

00:00:32.380 --> 00:00:36.050
Normally, this should be involved in.aws.credentials.

00:00:36.049 --> 00:00:39.759
I am putting it here in my local file only under aws/credentials.

00:00:39.759 --> 00:00:42.384
So it's clear.

00:00:42.384 --> 00:00:45.949
As you know, we should never share our access keys.

00:00:45.950 --> 00:00:49.429
Here I'm going to use another package which is Hadoop AWS,

00:00:49.429 --> 00:00:55.765
that is the library which is used for us to connect with S3.

00:00:55.765 --> 00:00:57.869
When we run that,

00:00:57.869 --> 00:01:00.234
we get the Spark also,

00:01:00.234 --> 00:01:05.810
session that's going to take a little bit longer than usual to load so be patient.

00:01:05.810 --> 00:01:09.545
Then actually we're going to give you this and you're supposed to

00:01:09.545 --> 00:01:13.659
upload it in your private bucket.

00:01:13.659 --> 00:01:17.215
So you're going to put it in another bucket not that one.

00:01:17.215 --> 00:01:23.600
We're going to add a data set that we used in Lesson 1 to explain dimensional modeling.

00:01:23.790 --> 00:01:26.905
We get the data frame, we print it,

00:01:26.905 --> 00:01:32.269
and seems that separator also is a semicolon here and it's a one-liner.

00:01:32.269 --> 00:01:33.920
So we need to do a little bit more.

00:01:33.920 --> 00:01:38.674
What do we do? We make the delimiter semicolon,

00:01:38.674 --> 00:01:39.980
we infer the schema,

00:01:39.980 --> 00:01:41.844
and hey we see there is a header,

00:01:41.844 --> 00:01:44.054
we don't want it to become headers data,

00:01:44.055 --> 00:01:46.770
and it's going to mess up the inference of the schema.

00:01:46.769 --> 00:01:48.739
So we make header equals true.

00:01:48.739 --> 00:01:52.819
We do that again and we print the schema and payment_id,

00:01:52.819 --> 00:01:56.184
customer_id, stuff_id. Looks good.

00:01:56.185 --> 00:02:02.314
Now we still have this idea of a string, payment string here.

00:02:02.314 --> 00:02:06.170
That's a date, but got loaded as string.

00:02:06.170 --> 00:02:11.064
Fine. No biggie, the problem is as follows,

00:02:11.064 --> 00:02:12.909
when you do this,

00:02:12.909 --> 00:02:16.699
this also might take a long time because you know what's happening.

00:02:16.699 --> 00:02:19.384
In all the past exercises,

00:02:19.384 --> 00:02:21.579
we were loading the data,

00:02:21.580 --> 00:02:25.565
I mean we had the data on your local machine and we had

00:02:25.564 --> 00:02:29.134
the Jupyter Notebook on the same machine.

00:02:29.134 --> 00:02:31.639
So actually we are loading the data locally.

00:02:31.639 --> 00:02:35.314
What we're doing now is we're loading the data all the way from

00:02:35.314 --> 00:02:40.224
Amazon and into the memory of the Jupyter Notebook machine.

00:02:40.224 --> 00:02:42.354
That actually works.

00:02:42.354 --> 00:02:43.619
That code works,

00:02:43.620 --> 00:02:49.034
but I really do not advise you to use that in production at all.

00:02:49.034 --> 00:02:54.049
That would only work if you are using a machine inside Amazon,

00:02:54.050 --> 00:02:57.085
and also in the same region,

00:02:57.085 --> 00:03:04.395
probably in the same region and same availability zone as your S3 bucket.

00:03:04.395 --> 00:03:11.675
But anyhow, here we're just doing it to show or prove that actually Spark can deal

00:03:11.675 --> 00:03:19.925
with S3 as explained in the lesson and also that we can do schema on it,

00:03:19.925 --> 00:03:23.939
and sometimes we need to work a little bit more.

