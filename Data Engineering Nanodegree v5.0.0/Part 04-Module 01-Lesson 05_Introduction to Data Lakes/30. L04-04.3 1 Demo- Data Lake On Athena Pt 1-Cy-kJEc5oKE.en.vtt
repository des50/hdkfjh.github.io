WEBVTT
Kind: captions
Language: en

00:00:00.020 --> 00:00:02.265
Now in this last demo,

00:00:02.265 --> 00:00:08.535
we're going to show how to create a service with Athena or a data lake with Athena,

00:00:08.535 --> 00:00:11.925
where we're going to have all our data in S3,

00:00:11.925 --> 00:00:15.839
and we want to query this data in a scalable way without

00:00:15.839 --> 00:00:19.730
even creating an EMR cluster or any EC2 machines.

00:00:19.730 --> 00:00:28.304
But per query, we're going to run our code on serverless AWS Lambda resources.

00:00:28.304 --> 00:00:30.914
So the first thing is to go and,

00:00:30.914 --> 00:00:33.629
actually this is post work if I can delete,

00:00:33.630 --> 00:00:41.429
and I go and say "Create table," from an S3 bucket data,

00:00:41.429 --> 00:00:45.125
and I can also say actually from a glue crawler,

00:00:45.125 --> 00:00:50.575
and I will go to the option of glue crawler.

00:00:50.575 --> 00:00:55.190
This will take us through the Amazon glue service.

00:00:55.189 --> 00:00:58.144
This one actually we can say,

00:00:58.145 --> 00:01:08.629
I'm going to call it MyDataLakeCrawler, and click "Next."

00:01:08.629 --> 00:01:13.049
So this is S3 and it's in my account.

00:01:13.870 --> 00:01:16.670
If this was a public account,

00:01:16.670 --> 00:01:18.515
if someone else's account,

00:01:18.515 --> 00:01:20.930
even if it's public, you will not be able to list it.

00:01:20.930 --> 00:01:24.155
So it's better to have this data in your accounts.

00:01:24.155 --> 00:01:30.450
So, I have my data here for pagila in this bucket.

00:01:30.450 --> 00:01:33.689
Next, do you need any other data?

00:01:33.689 --> 00:01:38.489
No, I don't. It's going to ask for a role.

00:01:38.489 --> 00:01:43.659
Actually, you can say MyDataLakeRole,

00:01:44.930 --> 00:01:48.600
and it creates that for you,

00:01:48.599 --> 00:01:51.614
and the frequency is run on demand.

00:01:51.614 --> 00:01:57.604
But you can also schedule this to be updated periodically.

00:01:57.605 --> 00:01:59.975
So it will run on demand when we click it,

00:01:59.974 --> 00:02:04.924
and the crawlers output is going to be into a new database,

00:02:04.924 --> 00:02:08.659
and I will add a new database and call it,

00:02:08.659 --> 00:02:15.159
actually I'm going to call this pagila because this is the pagila database.

00:02:15.590 --> 00:02:19.039
Do I want to add a prefix to this table?

00:02:19.039 --> 00:02:23.614
No, I don't. So, seems like everything is correct.

00:02:23.615 --> 00:02:25.495
I finish this.

00:02:25.495 --> 00:02:28.319
With that, I created a crawler.

00:02:28.319 --> 00:02:37.069
Glue basically is an ETL tool that goes and creates crawlers and takes the data.

00:02:38.199 --> 00:02:43.884
In fact, all the schemas and avails that to us for us to query.

00:02:43.884 --> 00:02:45.799
Let's see how that works.

00:02:45.800 --> 00:02:49.750
I'll click that crawler and start running MyCrawler.

00:02:49.750 --> 00:02:52.680
This is a post Crawler of mine.

00:02:52.680 --> 00:02:58.170
So, we are now working with MyDataLakeCrawler that you've just seen,

00:02:58.169 --> 00:03:00.944
and it's now running.

00:03:00.944 --> 00:03:06.734
Is same starting, and after a while, it will be ready.

00:03:06.735 --> 00:03:08.820
It actually doesn't take much,

00:03:08.819 --> 00:03:10.935
it takes about one minute.

00:03:10.935 --> 00:03:12.960
When the crawler finishes,

00:03:12.960 --> 00:03:14.985
this is going to take about one time,

00:03:14.985 --> 00:03:20.690
we can actually go and take a look here at the tables.

00:03:20.689 --> 00:03:25.984
You will see that you have all the tables here.

00:03:25.985 --> 00:03:29.665
Take care that actually you have to put

00:03:29.664 --> 00:03:36.439
each CSV file in its old folder for it to be named correctly like that.

00:03:37.219 --> 00:03:42.895
Now, we actually, maybe we can also go and open here,

00:03:42.895 --> 00:03:49.314
and we will see that we have a schema inference exactly like a Spark.

00:03:49.314 --> 00:03:51.074
Also the payment date is a string,

00:03:51.074 --> 00:03:53.169
but we know we can deal with this.

