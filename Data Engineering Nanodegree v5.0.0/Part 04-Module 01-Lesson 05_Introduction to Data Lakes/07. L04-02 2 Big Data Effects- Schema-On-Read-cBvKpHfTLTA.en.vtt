WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.299
Second thing which we want to look at in the big data technologies is the schema-on-read.

00:00:06.299 --> 00:00:13.379
So, traditionally I mean if I will give you the choice today to try to query the data,

00:00:13.380 --> 00:00:19.425
any data, just even tabular data on a database or in plain files, what would you choose?

00:00:19.425 --> 00:00:24.060
Naturally, you would choose the database.

00:00:24.059 --> 00:00:26.445
It's much easier to just run a SQL query.

00:00:26.445 --> 00:00:29.655
However, the big data tools in the Hadoop ecosystem,

00:00:29.655 --> 00:00:32.954
the processing tools like Hive and Spark and so on,

00:00:32.954 --> 00:00:37.479
made it easy to work with plain files that were not inserted into the database.

00:00:37.479 --> 00:00:43.034
So, you did not need to create a database or insert files into the database.

00:00:43.034 --> 00:00:45.254
Even in Hive which was like SQL,

00:00:45.255 --> 00:00:49.525
you could refer to that as an external table and work with,

00:00:49.524 --> 00:00:51.960
and that was the idea of schema-on-read.

00:00:51.960 --> 00:00:55.870
So, the schema of a file,

00:00:55.869 --> 00:00:58.784
it's either inferred or specified,

00:00:58.784 --> 00:01:00.679
but the data is not inserted,

00:01:00.679 --> 00:01:07.939
it's just pointing to it to check against the specify schema and be able to work with it.

00:01:07.939 --> 00:01:09.814
Let's take a closer look here.

00:01:09.814 --> 00:01:12.500
So, in Spark, and I am sure you have

00:01:12.500 --> 00:01:18.629
seen already that like spark.read.csv and you say infer schema.

00:01:18.629 --> 00:01:20.304
The idea would be that,

00:01:20.305 --> 00:01:23.390
if you infer the schema you can see that some types

00:01:23.390 --> 00:01:28.219
are correctly inferred and some types like for example,

00:01:28.219 --> 00:01:32.929
the payment date is inferred as string for the lack of a better knowledge and simplicity,

00:01:32.930 --> 00:01:36.470
and then we might of course cost that

00:01:36.469 --> 00:01:41.569
afterwards or we might decide to not leave it up to Spark to guess this.

00:01:41.569 --> 00:01:44.629
Sometimes actually schema inference won't work,

00:01:44.629 --> 00:01:47.564
like if you have many types,

00:01:47.564 --> 00:01:52.609
the data is not very clean and you have many types in the same column.

00:01:52.609 --> 00:01:56.875
You have like a float and a double and a string,

00:01:56.875 --> 00:01:59.269
it can happen, and that can confuse Spark,

00:01:59.269 --> 00:02:01.625
and not always the schema inference would work.

00:02:01.625 --> 00:02:04.474
But then you can go and specify the schema,

00:02:04.474 --> 00:02:06.599
the column for each type here,

00:02:06.599 --> 00:02:09.849
and also you can say which is very important,

00:02:09.849 --> 00:02:14.004
what happens if I find the row which does not conform to the schema?

00:02:14.004 --> 00:02:17.814
You have also the policy of like drop malformed.

00:02:17.814 --> 00:02:23.349
So, you can say ignore it or you can say fail and stop the whole process or you

00:02:23.349 --> 00:02:28.719
can say just leave it and fill this row with nulls,

00:02:28.719 --> 00:02:35.344
so I would be able to know that how many are bads or malformed.

00:02:35.344 --> 00:02:37.180
Luckily, if you apply that,

00:02:37.180 --> 00:02:39.310
you get properly enforced types.

00:02:39.310 --> 00:02:43.090
So, not because it's schema-on-read that we're going to make things messy,

00:02:43.090 --> 00:02:45.159
we still have a little bit of control.

00:02:45.159 --> 00:02:48.969
Once the data is pointed to by a schema,

00:02:48.969 --> 00:02:52.884
inferred or specified, and it's loaded into a data frame.

00:02:52.884 --> 00:02:54.669
Once you have a data frame,

00:02:54.669 --> 00:02:58.129
you can just go and do direct querying on the fly,

00:02:58.129 --> 00:03:00.379
no databases no insertions.

00:03:00.379 --> 00:03:07.939
Even better that you can even write SQL by just taking that data frame and register it

00:03:07.939 --> 00:03:16.865
as a table and start doing normal grading as if this was a table in a database.

00:03:16.865 --> 00:03:22.600
While again, it's still querying on the fly and there are no databases, no insertions.

